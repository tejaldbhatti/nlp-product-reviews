{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa9a12",
   "metadata": {},
   "source": [
    "# Load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d89e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (34660, 21)\n",
      "Columns: ['id', 'name', 'asins', 'brand', 'categories', 'keys', 'manufacturer', 'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs', 'reviews.text', 'reviews.title', 'reviews.userCity', 'reviews.userProvince', 'reviews.username']\n",
      "\n",
      "First few rows:\n",
      "                     id                                               name  \\\n",
      "0  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "1  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "2  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "3  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "4  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "\n",
      "        asins   brand                                         categories  \\\n",
      "0  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "1  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "2  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "3  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "4  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "\n",
      "                                                keys manufacturer  \\\n",
      "0  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "1  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "2  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "3  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "4  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "\n",
      "               reviews.date     reviews.dateAdded  \\\n",
      "0  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "1  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "2  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "3  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "4  2017-01-12T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "\n",
      "                                    reviews.dateSeen  ... reviews.doRecommend  \\\n",
      "0  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "1  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "2  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "3  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "4  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "\n",
      "  reviews.id  reviews.numHelpful  reviews.rating  \\\n",
      "0        NaN                 0.0             5.0   \n",
      "1        NaN                 0.0             5.0   \n",
      "2        NaN                 0.0             5.0   \n",
      "3        NaN                 0.0             4.0   \n",
      "4        NaN                 0.0             5.0   \n",
      "\n",
      "                                  reviews.sourceURLs  \\\n",
      "0  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "1  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "2  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "3  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "4  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "\n",
      "                                        reviews.text  \\\n",
      "0  This product so far has not disappointed. My c...   \n",
      "1  great for beginner or experienced person. Boug...   \n",
      "2  Inexpensive tablet for him to use and learn on...   \n",
      "3  I've had my Fire HD 8 two weeks now and I love...   \n",
      "4  I bought this for my grand daughter when she c...   \n",
      "\n",
      "                             reviews.title reviews.userCity  \\\n",
      "0                                   Kindle              NaN   \n",
      "1                                very fast              NaN   \n",
      "2  Beginner tablet for our 9 year old son.              NaN   \n",
      "3                                  Good!!!              NaN   \n",
      "4                Fantastic Tablet for kids              NaN   \n",
      "\n",
      "   reviews.userProvince  reviews.username  \n",
      "0                   NaN           Adapter  \n",
      "1                   NaN            truman  \n",
      "2                   NaN             DaveZ  \n",
      "3                   NaN            Shacks  \n",
      "4                   NaN         explore42  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yj/_2dcl_l16cl6dr1bp7h1j6hw0000gn/T/ipykernel_75661/1418534009.py:4: DtypeWarning: Columns (1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/1429_1.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/1429_1.csv')\n",
    "\n",
    "# Basic info about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4f2cd",
   "metadata": {},
   "source": [
    "# Examine the key columns for our tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49aaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” KEY COLUMNS ANALYSIS\n",
      "==================================================\n",
      "REVIEW TEXT (first 3 examples):\n",
      "1. Rating: 5.0 - Text: This product so far has not disappointed. My children love to use it and I like the ability to monit...\n",
      "2. Rating: 5.0 - Text: great for beginner or experienced person. Bought as a gift and she loves it...\n",
      "3. Rating: 5.0 - Text: Inexpensive tablet for him to use and learn on, step up from the NABI. He was thrilled with it, lear...\n",
      "\n",
      "RATING DISTRIBUTION:\n",
      "reviews.rating\n",
      "1.0      410\n",
      "2.0      402\n",
      "3.0     1499\n",
      "4.0     8541\n",
      "5.0    23775\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rating Statistics:\n",
      "count    34627.000000\n",
      "mean         4.584573\n",
      "std          0.735653\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max          5.000000\n",
      "Name: reviews.rating, dtype: float64\n",
      "\n",
      "CATEGORIES (first 5 examples):\n",
      "1. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "2. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "3. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "4. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "5. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "\n",
      "UNIQUE CATEGORIES COUNT: 41\n",
      "\n",
      "PRODUCTS:\n",
      "Unique products: 48\n",
      "Unique brands: 6\n",
      "Top 5 brands:\n",
      "brand\n",
      "Amazon                          28701\n",
      "Amazon Fire Tv                   5056\n",
      "Amazon Echo                       636\n",
      "Amazon Fire                       256\n",
      "Amazon Digital Services Inc.       10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DATA QUALITY:\n",
      "Missing values in key columns:\n",
      "  reviews.text: 1 missing (0.0%)\n",
      "  reviews.rating: 33 missing (0.1%)\n",
      "  categories: 0 missing (0.0%)\n",
      "  name: 6760 missing (19.5%)\n",
      "  brand: 0 missing (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” KEY COLUMNS ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. SENTIMENT ANALYSIS - Check reviews.text and reviews.rating\n",
    "print(\"REVIEW TEXT (first 3 examples):\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. Rating: {df['reviews.rating'].iloc[i]} - Text: {df['reviews.text'].iloc[i][:100]}...\")\n",
    "\n",
    "print(f\"\\nRATING DISTRIBUTION:\")\n",
    "print(df['reviews.rating'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nRating Statistics:\")\n",
    "print(df['reviews.rating'].describe())\n",
    "\n",
    "# 2. CATEGORY CLUSTERING - Check categories column\n",
    "print(f\"\\nCATEGORIES (first 5 examples):\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {df['categories'].iloc[i]}\")\n",
    "\n",
    "print(f\"\\nUNIQUE CATEGORIES COUNT: {df['categories'].nunique()}\")\n",
    "\n",
    "# 3. PRODUCT ANALYSIS - Check name and brand\n",
    "print(f\"\\nPRODUCTS:\")\n",
    "print(f\"Unique products: {df['name'].nunique()}\")\n",
    "print(f\"Unique brands: {df['brand'].nunique()}\")\n",
    "print(f\"Top 5 brands:\")\n",
    "print(df['brand'].value_counts().head())\n",
    "\n",
    "# 4. DATA QUALITY CHECK\n",
    "print(f\"\\nDATA QUALITY:\")\n",
    "print(f\"Missing values in key columns:\")\n",
    "key_columns = ['reviews.text', 'reviews.rating', 'categories', 'name', 'brand']\n",
    "for col in key_columns:\n",
    "    missing = df[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing} missing ({missing/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b67f23",
   "metadata": {},
   "source": [
    "# Key Insights:\n",
    "\n",
    "## Sentiment Analysis: Great rating distribution (mostly 4-5 stars), very few missing reviews\n",
    "- Category Clustering: 41 unique categories to reduce to 4-6 meta-categories\n",
    "- Products: 48 unique products, dominated by Amazon products\n",
    "- Data Quality: Very clean dataset, minimal missing values\n",
    "- Inbalanced class in 1.0 and 2.0 ratings in the dataset (most belong to the 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab2620",
   "metadata": {},
   "source": [
    "# Categories Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23afa460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORY ANALYSIS FOR CLUSTERING\n",
      "==================================================\n",
      "Most common category terms:\n",
      "  Electronics: 42,291\n",
      "  Computers & Tablets: 21,719\n",
      "  Tablets: 21,383\n",
      "  All Tablets: 18,413\n",
      "  iPad & Tablets: 17,784\n",
      "  Electronics Features: 16,926\n",
      "  Fire Tablets: 16,303\n",
      "  Home: 14,597\n",
      "  Kindle Store: 12,886\n",
      "  Amazon Devices: 12,691\n",
      "  Featured Brands: 12,647\n",
      "  TVs Entertainment: 11,682\n",
      "  Holiday Shop: 11,682\n",
      "  Frys: 11,615\n",
      "  Tech Toys: 11,608\n",
      "\n",
      "SAMPLE CATEGORY STRINGS:\n",
      " 1. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      " 2. eBook Readers,Kindle E-readers,Computers & Tablets,E-Readers & Accessories,E-Readers\n",
      " 3. Electronics,eBook Readers & Accessories,Covers,Kindle Store,Amazon Device Accessories,Kindle E-Reader Accessories,Kindle (5th Generation) Accessories,Kindle (5th Generation) Covers\n",
      " 4. Kindle Store,Amazon Devices,Electronics\n",
      " 5. Tablets,Fire Tablets,Electronics,Computers,Computer Components,Hard Drives & Storage,Computers & Tablets,All Tablets\n",
      " 6. Tablets,Fire Tablets,Computers & Tablets,All Tablets\n",
      " 7. Amazon Devices & Accessories,Amazon Device Accessories,Power Adapters & Cables,Kindle Store,Kindle E-Reader Accessories,Kindle Paperwhite Accessories\n",
      " 8. Electronics,iPad & Tablets,All Tablets,Computers/Tablets & Networking,Tablets & eBook Readers,Computers & Tablets,E-Readers & Accessories,E-Readers,Used:Computers Accessories,Used:Tablets,Computers,iPads Tablets,Kindle E-readers,Electronics Features\n",
      " 9. Computers/Tablets & Networking,Tablets & eBook Readers,Electronics,eBook Readers & Accessories,eBook Readers\n",
      "10. Fire Tablets,Tablets,Computers & Tablets,All Tablets,Electronics, Tech Toys, Movies, Music,Electronics,iPad & Tablets,Android Tablets,Frys\n",
      "\n",
      "SAMPLE PRODUCT NAMES:\n",
      " 1. All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta\n",
      " 2. Kindle Oasis E-reader with Leather Charging Cover - Merlot, 6 High-Resolution Display (300 ppi), Wi-Fi - Includes Special Offers,,\n",
      " 3. Amazon Kindle Lighted Leather Cover,,,\n",
      "Amazon Kindle Lighted Leather Cover,,,\n",
      " 4. Amazon Kindle Lighted Leather Cover,,,\n",
      "Kindle Keyboard,,,\n",
      " 5. Kindle Keyboard,,,\n",
      "Kindle Keyboard,,,\n",
      " 6. All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 32 GB - Includes Special Offers, Magenta\n",
      " 7. Fire HD 8 Tablet with Alexa, 8 HD Display, 32 GB, Tangerine - with Special Offers,\n",
      " 8. Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\n",
      "Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\n",
      " 9. All-New Kindle E-reader - Black, 6 Glare-Free Touchscreen Display, Wi-Fi -  Includes Special Offers,,\n",
      "10. Amazon Kindle Fire Hd (3rd Generation) 8gb,,,\n",
      "Amazon Kindle Fire Hd (3rd Generation) 8gb,,,\n",
      "\n",
      "TEXT LENGTH ANALYSIS:\n",
      "Average review length: 159 characters\n",
      "Median review length: 106 characters\n",
      "Max review length: 10,670 characters\n",
      "\n",
      "Data exploration complete!\n",
      "Ready for preprocessing and model building\n"
     ]
    }
   ],
   "source": [
    "# Categories for clustering task\n",
    "print(\"CATEGORY ANALYSIS FOR CLUSTERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split categories and analyze individual category terms\n",
    "all_categories = []\n",
    "for cat_string in df['categories'].dropna():\n",
    "    categories = cat_string.split(',')\n",
    "    all_categories.extend([cat.strip() for cat in categories])\n",
    "\n",
    "from collections import Counter\n",
    "category_counts = Counter(all_categories)\n",
    "\n",
    "print(f\"Most common category terms:\")\n",
    "for cat, count in category_counts.most_common(15):\n",
    "    print(f\"  {cat}: {count:,}\")\n",
    "\n",
    "print(f\"\\nSAMPLE CATEGORY STRINGS:\")\n",
    "unique_categories = df['categories'].unique()[:10]\n",
    "for i, cat in enumerate(unique_categories, 1):\n",
    "    print(f\"{i:2d}. {cat}\")\n",
    "\n",
    "# Analyze product names to understand what we're working with\n",
    "print(f\"\\nSAMPLE PRODUCT NAMES:\")\n",
    "unique_products = df['name'].dropna().unique()[:10]\n",
    "for i, product in enumerate(unique_products, 1):\n",
    "    print(f\"{i:2d}. {product}\")\n",
    "\n",
    "# Check text length for preprocessing planning\n",
    "df['text_length'] = df['reviews.text'].fillna('').str.len()\n",
    "print(f\"\\nTEXT LENGTH ANALYSIS:\")\n",
    "print(f\"Average review length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"Median review length: {df['text_length'].median():.0f} characters\")\n",
    "print(f\"Max review length: {df['text_length'].max():,} characters\")\n",
    "\n",
    "print(f\"\\nData exploration complete!\")\n",
    "print(f\"Ready for preprocessing and model building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26396cd6",
   "metadata": {},
   "source": [
    "# Create the meta-categories & start pre-processing\n",
    "- Create 5 meaningful meta-categories based on your data\n",
    "- Convert ratings to sentiment labels (negative/neutral/positive)\n",
    "- Show the distribution of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4fa476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING META-CATEGORIES\n",
      "==================================================\n",
      "META-CATEGORY DISTRIBUTION:\n",
      "  E-Readers: 18,958 (54.7%)\n",
      "  Tablets: 15,653 (45.2%)\n",
      "  Other Electronics: 34 (0.1%)\n",
      "  Accessories: 8 (0.0%)\n",
      "  Smart Home & Entertainment: 7 (0.0%)\n",
      "\n",
      "EXAMPLES BY META-CATEGORY:\n",
      "  Tablets: All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta\n",
      "  E-Readers: Kindle Oasis E-reader with Leather Charging Cover - Merlot, 6 High-Resolution Display (300 ppi), Wi-Fi - Includes Special Offers,,\n",
      "  Other Electronics: Brand New Amazon Kindle Fire 16gb 7 Ips Display Tablet Wifi 16 Gb Blue,,,\n",
      "  Smart Home & Entertainment: No product names available\n",
      "  Accessories: No product names available\n",
      "\n",
      "SENTIMENT DISTRIBUTION:\n",
      "  positive: 32,316 (93.3%)\n",
      "  neutral: 1,499 (4.3%)\n",
      "  negative: 812 (2.3%)\n",
      "\n",
      "Meta-categories and sentiment labels created!\n",
      "Ready to start building models\n"
     ]
    }
   ],
   "source": [
    "# Define meta-categories based on the data analysis\n",
    "print(\"CREATING META-CATEGORIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def categorize_product(category_string, product_name):\n",
    "    \"\"\"\n",
    "    Classify products into meta-categories based on category strings and product names\n",
    "    \"\"\"\n",
    "    if pd.isna(category_string):\n",
    "        category_string = \"\"\n",
    "    if pd.isna(product_name):\n",
    "        product_name = \"\"\n",
    "    \n",
    "    cat_lower = category_string.lower()\n",
    "    prod_lower = product_name.lower()\n",
    "    \n",
    "    # E-readers (Kindle devices)\n",
    "    if any(term in cat_lower for term in ['kindle', 'ebook', 'e-reader']):\n",
    "        return \"E-Readers\"\n",
    "    \n",
    "    # Tablets (Fire tablets, iPads)\n",
    "    elif any(term in cat_lower for term in ['fire tablet', 'ipad', 'tablet']) and 'accessory' not in cat_lower:\n",
    "        return \"Tablets\"\n",
    "    \n",
    "    # Accessories (covers, chargers, cables)\n",
    "    elif any(term in cat_lower for term in ['cover', 'accessory', 'cable', 'charger', 'adapter']):\n",
    "        return \"Accessories\"\n",
    "    \n",
    "    # Smart Home/Entertainment (Echo, Fire TV)\n",
    "    elif any(term in cat_lower for term in ['echo', 'fire tv', 'entertainment', 'home']):\n",
    "        return \"Smart Home & Entertainment\"\n",
    "    \n",
    "    # Default fallback\n",
    "    else:\n",
    "        return \"Other Electronics\"\n",
    "\n",
    "# Apply categorization\n",
    "df['meta_category'] = df.apply(lambda row: categorize_product(row['categories'], row['name']), axis=1)\n",
    "\n",
    "# Check the distribution\n",
    "print(\"META-CATEGORY DISTRIBUTION:\")\n",
    "meta_dist = df['meta_category'].value_counts()\n",
    "for category, count in meta_dist.items():\n",
    "    print(f\"  {category}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nEXAMPLES BY META-CATEGORY:\")\n",
    "for category in df['meta_category'].unique():\n",
    "    # Fixed: properly check for available names\n",
    "    category_products = df[df['meta_category'] == category]['name'].dropna()\n",
    "    if len(category_products) > 0:\n",
    "        sample = category_products.iloc[0]\n",
    "    else:\n",
    "        sample = \"No product names available\"\n",
    "    print(f\"  {category}: {sample}\")\n",
    "\n",
    "# Prepare sentiment labels (convert ratings to sentiment)\n",
    "def rating_to_sentiment(rating):\n",
    "    \"\"\"Convert 1-5 star rating to sentiment label\"\"\"\n",
    "    if pd.isna(rating):\n",
    "        return None\n",
    "    elif rating <= 2:\n",
    "        return \"negative\"\n",
    "    elif rating == 3:\n",
    "        return \"neutral\" \n",
    "    else:  # rating >= 4\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['reviews.rating'].apply(rating_to_sentiment)\n",
    "\n",
    "print(f\"\\nSENTIMENT DISTRIBUTION:\")\n",
    "sentiment_dist = df['sentiment'].value_counts()\n",
    "for sentiment, count in sentiment_dist.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/len(df.dropna(subset=['sentiment']))*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMeta-categories and sentiment labels created!\")\n",
    "print(f\"Ready to start building models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551e721",
   "metadata": {},
   "source": [
    "# There seems to be imbalanced distribution:\n",
    "- Meta-category: Mostly E-Readers and Tablets (98.9%)\n",
    "- Heavily biased: 93.3% are positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da0f58",
   "metadata": {},
   "source": [
    "## Review imbalences deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68e259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING CLASS IMBALANCE\n",
      "==================================================\n",
      "CURRENT SENTIMENT DISTRIBUTION:\n",
      "  positive: 32,315 (93.3%)\n",
      "  neutral: 1,499 (4.3%)\n",
      "  negative: 812 (2.3%)\n",
      "\n",
      "SENTIMENT BY META-CATEGORY:\n",
      "sentiment                   negative  neutral  positive\n",
      "meta_category                                          \n",
      "Accessories                        0        0         8\n",
      "E-Readers                        363      614     17957\n",
      "Other Electronics                  4        0        20\n",
      "Smart Home & Entertainment         1        0         6\n",
      "Tablets                          444      885     14324\n",
      "\n",
      "DETAILED RATING DISTRIBUTION:\n",
      "  1.0 stars -> negative: 410 (1.2%)\n",
      "  2.0 stars -> negative: 402 (1.2%)\n",
      "  3.0 stars -> neutral: 1,499 (4.3%)\n",
      "  4.0 stars -> positive: 8,541 (24.7%)\n",
      "  5.0 stars -> positive: 23,774 (68.7%)\n",
      "\n",
      "OPTIONS FOR HANDLING IMBALANCE:\n",
      "1. Use stratified sampling to balance classes\n",
      "2. Use class weights in the model\n",
      "3. Focus on binary classification (positive vs negative+neutral)\n",
      "4. Use different evaluation metrics (precision, recall, F1)\n",
      "\n",
      "CREATING BALANCED SAMPLE FOR TESTING:\n",
      "Smallest class has 812 samples\n",
      "\n",
      "BALANCED SAMPLE DISTRIBUTION:\n",
      "  negative: 812 (33.3%)\n",
      "  neutral: 812 (33.3%)\n",
      "  positive: 812 (33.3%)\n",
      "\n",
      "Balanced sample size: 2,436 (vs original 34,626)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yj/_2dcl_l16cl6dr1bp7h1j6hw0000gn/T/ipykernel_75661/3420261739.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = df_clean.groupby('sentiment').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the processed directory first\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Analyze the imbalance problem\n",
    "print(\"ANALYZING CLASS IMBALANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Current distribution\n",
    "print(\"CURRENT SENTIMENT DISTRIBUTION:\")\n",
    "sentiment_counts = df_clean['sentiment'].value_counts()\n",
    "total = len(df_clean)\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/total*100:.1f}%)\")\n",
    "\n",
    "# Check if imbalance varies by category\n",
    "print(f\"\\nSENTIMENT BY META-CATEGORY:\")\n",
    "sentiment_by_category = df_clean.groupby(['meta_category', 'sentiment']).size().unstack(fill_value=0)\n",
    "print(sentiment_by_category)\n",
    "\n",
    "# Look at the rating distribution in more detail\n",
    "print(f\"\\nDETAILED RATING DISTRIBUTION:\")\n",
    "rating_counts = df_clean['reviews.rating'].value_counts().sort_index()\n",
    "for rating, count in rating_counts.items():\n",
    "    sentiment = rating_to_sentiment(rating)\n",
    "    print(f\"  {rating} stars -> {sentiment}: {count:,} ({count/total*100:.1f}%)\")\n",
    "\n",
    "# Options for handling imbalance\n",
    "print(f\"\\nOPTIONS FOR HANDLING IMBALANCE:\")\n",
    "print(\"1. Use stratified sampling to balance classes\")\n",
    "print(\"2. Use class weights in the model\")\n",
    "print(\"3. Focus on binary classification (positive vs negative+neutral)\")\n",
    "print(\"4. Use different evaluation metrics (precision, recall, F1)\")\n",
    "\n",
    "# Let's create a more balanced sample for initial testing\n",
    "print(f\"\\nCREATING BALANCED SAMPLE FOR TESTING:\")\n",
    "\n",
    "# Sample equal amounts from each class (limited by smallest class)\n",
    "min_class_size = sentiment_counts.min()\n",
    "print(f\"Smallest class has {min_class_size} samples\")\n",
    "\n",
    "# Create balanced sample\n",
    "balanced_df = df_clean.groupby('sentiment').apply(\n",
    "    lambda x: x.sample(n=min(min_class_size, len(x)), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nBALANCED SAMPLE DISTRIBUTION:\")\n",
    "balanced_counts = balanced_df['sentiment'].value_counts()\n",
    "balanced_total = len(balanced_df)\n",
    "for sentiment, count in balanced_counts.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/balanced_total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nBalanced sample size: {len(balanced_df):,} (vs original {len(df_clean):,})\")\n",
    "\n",
    "# Save the data\n",
    "df_clean.to_csv('data/processed/cleaned_reviews.csv', index=False)\n",
    "balanced_df.to_csv('data/processed/balanced_reviews.csv', index=False)\n",
    "print(f\"\\nData saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402d73c",
   "metadata": {},
   "source": [
    "# Sentiment analysis model using LogisticRegression\n",
    "(sample model below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING SENTIMENT ANALYSIS MODEL\n",
      "==================================================\n",
      "TESTING WITH BALANCED DATASET\n",
      "------------------------------\n",
      "Training set: 1948 samples\n",
      "Test set: 488 samples\n",
      "TF-IDF matrix shape: (1948, 4809)\n",
      "\n",
      "MODEL EVALUATION (BALANCED DATASET):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.65      0.64       162\n",
      "     neutral       0.54      0.55      0.54       163\n",
      "    positive       0.73      0.71      0.72       163\n",
      "\n",
      "    accuracy                           0.63       488\n",
      "   macro avg       0.63      0.63      0.63       488\n",
      "weighted avg       0.63      0.63      0.63       488\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "         neg  neu  pos\n",
      "Actual neg 105  42  15\n",
      "       neu  46  89  28\n",
      "       pos  15  33 115\n",
      "\n",
      "SAMPLE PREDICTIONS:\n",
      "1. Text: 'This product is amazing! I love it so much.'\n",
      "   Prediction: positive\n",
      "   Probabilities: neg=0.072, neu=0.089, pos=0.839\n",
      "\n",
      "2. Text: 'It's okay, nothing special but does the job.'\n",
      "   Prediction: neutral\n",
      "   Probabilities: neg=0.105, neu=0.795, pos=0.100\n",
      "\n",
      "3. Text: 'Terrible quality, waste of money. Don't buy this.'\n",
      "   Prediction: negative\n",
      "   Probabilities: neg=0.888, neu=0.072, pos=0.041\n",
      "\n",
      "BASELINE MODEL COMPLETE!\n",
      "Next step: Test with class weights on full dataset\n"
     ]
    }
   ],
   "source": [
    "# Build our first sentiment analysis model\n",
    "print(\"BUILDING SENTIMENT ANALYSIS MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Start with the balanced dataset for initial testing\n",
    "print(\"TESTING WITH BALANCED DATASET\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Prepare the data\n",
    "X_balanced = balanced_df['reviews.text_clean'].tolist()\n",
    "y_balanced = balanced_df['sentiment'].tolist()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # unigrams and bigrams\n",
    "    min_df=2,  # ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95  # ignore terms that appear in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"\\nMODEL EVALUATION (BALANCED DATASET):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nCONFUSION MATRIX:\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['negative', 'neutral', 'positive'])\n",
    "print(\"           Predicted\")\n",
    "print(\"         neg  neu  pos\")\n",
    "print(f\"Actual neg {cm[0][0]:3d} {cm[0][1]:3d} {cm[0][2]:3d}\")\n",
    "print(f\"       neu {cm[1][0]:3d} {cm[1][1]:3d} {cm[1][2]:3d}\")\n",
    "print(f\"       pos {cm[2][0]:3d} {cm[2][1]:3d} {cm[2][2]:3d}\")\n",
    "\n",
    "# Test on some sample texts\n",
    "print(f\"\\nSAMPLE PREDICTIONS:\")\n",
    "sample_texts = [\n",
    "    \"This product is amazing! I love it so much.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Terrible quality, waste of money. Don't buy this.\"\n",
    "]\n",
    "\n",
    "sample_tfidf = vectorizer.transform(sample_texts)\n",
    "sample_predictions = model.predict(sample_tfidf)\n",
    "sample_probabilities = model.predict_proba(sample_tfidf)\n",
    "\n",
    "for i, (text, pred, prob) in enumerate(zip(sample_texts, sample_predictions, sample_probabilities)):\n",
    "    print(f\"{i+1}. Text: '{text}'\")\n",
    "    print(f\"   Prediction: {pred}\")\n",
    "    print(f\"   Probabilities: neg={prob[0]:.3f}, neu={prob[1]:.3f}, pos={prob[2]:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"BASELINE MODEL COMPLETE!\")\n",
    "print(\"Next step: Test with class weights on full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc73b5",
   "metadata": {},
   "source": [
    "## The model is working well on the balanced dataset (63% accuracy, good sample predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020440c3",
   "metadata": {},
   "source": [
    "# Test on full imbalanced dataset (with class weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ab6719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING WITH FULL DATASET + CLASS WEIGHTS\n",
      "==================================================\n",
      "Full training set: 27,700 samples\n",
      "Full test set: 6,926 samples\n",
      "Class weights: {np.str_('negative'): np.float64(14.205128205128204), np.str_('neutral'): np.float64(7.700861829302196), np.str_('positive'): np.float64(0.3571750931620956)}\n",
      "Full TF-IDF matrix shape: (27700, 10000)\n",
      "\n",
      "MODEL EVALUATION (FULL DATASET WITH CLASS WEIGHTS):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.56      0.38       162\n",
      "     neutral       0.16      0.40      0.23       300\n",
      "    positive       0.97      0.88      0.92      6464\n",
      "\n",
      "    accuracy                           0.85      6926\n",
      "   macro avg       0.47      0.61      0.51      6926\n",
      "weighted avg       0.92      0.85      0.88      6926\n",
      "\n",
      "\n",
      "PREDICTION DISTRIBUTION:\n",
      "Actual vs Predicted:\n",
      "  negative: 2.3% actual, 4.6% predicted\n",
      "  neutral: 4.3% actual, 10.9% predicted\n",
      "  positive: 93.3% actual, 84.5% predicted\n",
      "\n",
      "SAMPLE PREDICTIONS (WEIGHTED MODEL):\n",
      "1. Text: 'This product is amazing! I love it so much.'\n",
      "   Prediction: positive\n",
      "   Probabilities: neg=0.008, neu=0.017, pos=0.975\n",
      "\n",
      "2. Text: 'It's okay, nothing special but does the job.'\n",
      "   Prediction: neutral\n",
      "   Probabilities: neg=0.014, neu=0.921, pos=0.065\n",
      "\n",
      "3. Text: 'Terrible quality, waste of money. Don't buy this.'\n",
      "   Prediction: negative\n",
      "   Probabilities: neg=0.991, neu=0.007, pos=0.002\n",
      "\n",
      "SENTIMENT ANALYSIS MODEL COMPLETE!\n",
      "Ready for next task: Category clustering\n"
     ]
    }
   ],
   "source": [
    "# Test model with class weights on full dataset\n",
    "print(\"TESTING WITH FULL DATASET + CLASS WEIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Prepare full dataset\n",
    "X_full = df_clean['reviews.text_clean'].tolist()\n",
    "y_full = df_clean['sentiment'].tolist()\n",
    "\n",
    "# Split full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"Full training set: {len(X_train_full):,} samples\")\n",
    "print(f\"Full test set: {len(X_test_full):,} samples\")\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_full),\n",
    "    y=y_train_full\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train_full), class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# Create new vectorizer for full dataset\n",
    "vectorizer_full = TfidfVectorizer(\n",
    "    max_features=10000,  # More features for larger dataset\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,  # Higher threshold for larger dataset\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_full_tfidf = vectorizer_full.fit_transform(X_train_full)\n",
    "X_test_full_tfidf = vectorizer_full.transform(X_test_full)\n",
    "\n",
    "print(f\"Full TF-IDF matrix shape: {X_train_full_tfidf.shape}\")\n",
    "\n",
    "# Train model with class weights\n",
    "model_weighted = LogisticRegression(\n",
    "    random_state=42, \n",
    "    max_iter=1000, \n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "model_weighted.fit(X_train_full_tfidf, y_train_full)\n",
    "\n",
    "# Predictions\n",
    "y_pred_full = model_weighted.predict(X_test_full_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\nMODEL EVALUATION (FULL DATASET WITH CLASS WEIGHTS):\")\n",
    "print(classification_report(y_test_full, y_pred_full))\n",
    "\n",
    "# Show distribution of predictions vs actual\n",
    "print(f\"\\nPREDICTION DISTRIBUTION:\")\n",
    "from collections import Counter\n",
    "actual_dist = Counter(y_test_full)\n",
    "pred_dist = Counter(y_pred_full)\n",
    "\n",
    "print(\"Actual vs Predicted:\")\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    actual_pct = actual_dist[sentiment] / len(y_test_full) * 100\n",
    "    pred_pct = pred_dist[sentiment] / len(y_pred_full) * 100\n",
    "    print(f\"  {sentiment}: {actual_pct:.1f}% actual, {pred_pct:.1f}% predicted\")\n",
    "\n",
    "# Test the same sample texts\n",
    "print(f\"\\nSAMPLE PREDICTIONS (WEIGHTED MODEL):\")\n",
    "sample_tfidf_full = vectorizer_full.transform(sample_texts)\n",
    "sample_pred_full = model_weighted.predict(sample_tfidf_full)\n",
    "sample_prob_full = model_weighted.predict_proba(sample_tfidf_full)\n",
    "\n",
    "for i, (text, pred, prob) in enumerate(zip(sample_texts, sample_pred_full, sample_prob_full)):\n",
    "    print(f\"{i+1}. Text: '{text}'\")\n",
    "    print(f\"   Prediction: {pred}\")\n",
    "    print(f\"   Probabilities: neg={prob[0]:.3f}, neu={prob[1]:.3f}, pos={prob[2]:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"SENTIMENT ANALYSIS MODEL COMPLETE!\")\n",
    "print(\"Ready for next task: Category clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5380c",
   "metadata": {},
   "source": [
    "## Sentiment analysis model is working well. The class weights helped balance the predictions - it's now predicting more negatives and neutrals instead of everything being positive. The sample predictions look very accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
