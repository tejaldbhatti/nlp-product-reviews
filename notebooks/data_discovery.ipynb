{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa9a12",
   "metadata": {},
   "source": [
    "# Load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d89e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (34660, 21)\n",
      "Columns: ['id', 'name', 'asins', 'brand', 'categories', 'keys', 'manufacturer', 'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs', 'reviews.text', 'reviews.title', 'reviews.userCity', 'reviews.userProvince', 'reviews.username']\n",
      "\n",
      "First few rows:\n",
      "                     id                                               name  \\\n",
      "0  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "1  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "2  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "3  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "4  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
      "\n",
      "        asins   brand                                         categories  \\\n",
      "0  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "1  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "2  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "3  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "4  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
      "\n",
      "                                                keys manufacturer  \\\n",
      "0  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "1  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "2  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "3  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "4  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
      "\n",
      "               reviews.date     reviews.dateAdded  \\\n",
      "0  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "1  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "2  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "3  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "4  2017-01-12T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
      "\n",
      "                                    reviews.dateSeen  ... reviews.doRecommend  \\\n",
      "0  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "1  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "2  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "3  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "4  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
      "\n",
      "  reviews.id  reviews.numHelpful  reviews.rating  \\\n",
      "0        NaN                 0.0             5.0   \n",
      "1        NaN                 0.0             5.0   \n",
      "2        NaN                 0.0             5.0   \n",
      "3        NaN                 0.0             4.0   \n",
      "4        NaN                 0.0             5.0   \n",
      "\n",
      "                                  reviews.sourceURLs  \\\n",
      "0  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "1  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "2  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "3  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "4  http://reviews.bestbuy.com/3545/5620406/review...   \n",
      "\n",
      "                                        reviews.text  \\\n",
      "0  This product so far has not disappointed. My c...   \n",
      "1  great for beginner or experienced person. Boug...   \n",
      "2  Inexpensive tablet for him to use and learn on...   \n",
      "3  I've had my Fire HD 8 two weeks now and I love...   \n",
      "4  I bought this for my grand daughter when she c...   \n",
      "\n",
      "                             reviews.title reviews.userCity  \\\n",
      "0                                   Kindle              NaN   \n",
      "1                                very fast              NaN   \n",
      "2  Beginner tablet for our 9 year old son.              NaN   \n",
      "3                                  Good!!!              NaN   \n",
      "4                Fantastic Tablet for kids              NaN   \n",
      "\n",
      "   reviews.userProvince  reviews.username  \n",
      "0                   NaN           Adapter  \n",
      "1                   NaN            truman  \n",
      "2                   NaN             DaveZ  \n",
      "3                   NaN            Shacks  \n",
      "4                   NaN         explore42  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yj/_2dcl_l16cl6dr1bp7h1j6hw0000gn/T/ipykernel_97199/1418534009.py:4: DtypeWarning: Columns (1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/1429_1.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/1429_1.csv')\n",
    "\n",
    "# Basic info about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4f2cd",
   "metadata": {},
   "source": [
    "# Examine the key columns for our tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49aaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” KEY COLUMNS ANALYSIS\n",
      "==================================================\n",
      "REVIEW TEXT (first 3 examples):\n",
      "1. Rating: 5.0 - Text: This product so far has not disappointed. My children love to use it and I like the ability to monit...\n",
      "2. Rating: 5.0 - Text: great for beginner or experienced person. Bought as a gift and she loves it...\n",
      "3. Rating: 5.0 - Text: Inexpensive tablet for him to use and learn on, step up from the NABI. He was thrilled with it, lear...\n",
      "\n",
      "RATING DISTRIBUTION:\n",
      "reviews.rating\n",
      "1.0      410\n",
      "2.0      402\n",
      "3.0     1499\n",
      "4.0     8541\n",
      "5.0    23775\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rating Statistics:\n",
      "count    34627.000000\n",
      "mean         4.584573\n",
      "std          0.735653\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max          5.000000\n",
      "Name: reviews.rating, dtype: float64\n",
      "\n",
      "CATEGORIES (first 5 examples):\n",
      "1. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "2. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "3. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "4. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "5. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "\n",
      "UNIQUE CATEGORIES COUNT: 41\n",
      "\n",
      "PRODUCTS:\n",
      "Unique products: 48\n",
      "Unique brands: 6\n",
      "Top 5 brands:\n",
      "brand\n",
      "Amazon                          28701\n",
      "Amazon Fire Tv                   5056\n",
      "Amazon Echo                       636\n",
      "Amazon Fire                       256\n",
      "Amazon Digital Services Inc.       10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DATA QUALITY:\n",
      "Missing values in key columns:\n",
      "  reviews.text: 1 missing (0.0%)\n",
      "  reviews.rating: 33 missing (0.1%)\n",
      "  categories: 0 missing (0.0%)\n",
      "  name: 6760 missing (19.5%)\n",
      "  brand: 0 missing (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” KEY COLUMNS ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. SENTIMENT ANALYSIS - Check reviews.text and reviews.rating\n",
    "print(\"REVIEW TEXT (first 3 examples):\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. Rating: {df['reviews.rating'].iloc[i]} - Text: {df['reviews.text'].iloc[i][:100]}...\")\n",
    "\n",
    "print(f\"\\nRATING DISTRIBUTION:\")\n",
    "print(df['reviews.rating'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nRating Statistics:\")\n",
    "print(df['reviews.rating'].describe())\n",
    "\n",
    "# 2. CATEGORY CLUSTERING - Check categories column\n",
    "print(f\"\\nCATEGORIES (first 5 examples):\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {df['categories'].iloc[i]}\")\n",
    "\n",
    "print(f\"\\nUNIQUE CATEGORIES COUNT: {df['categories'].nunique()}\")\n",
    "\n",
    "# 3. PRODUCT ANALYSIS - Check name and brand\n",
    "print(f\"\\nPRODUCTS:\")\n",
    "print(f\"Unique products: {df['name'].nunique()}\")\n",
    "print(f\"Unique brands: {df['brand'].nunique()}\")\n",
    "print(f\"Top 5 brands:\")\n",
    "print(df['brand'].value_counts().head())\n",
    "\n",
    "# 4. DATA QUALITY CHECK\n",
    "print(f\"\\nDATA QUALITY:\")\n",
    "print(f\"Missing values in key columns:\")\n",
    "key_columns = ['reviews.text', 'reviews.rating', 'categories', 'name', 'brand']\n",
    "for col in key_columns:\n",
    "    missing = df[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing} missing ({missing/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b67f23",
   "metadata": {},
   "source": [
    "# Key Insights:\n",
    "\n",
    "## Sentiment Analysis: Great rating distribution (mostly 4-5 stars), very few missing reviews\n",
    "- Category Clustering: 41 unique categories to reduce to 4-6 meta-categories\n",
    "- Products: 48 unique products, dominated by Amazon products\n",
    "- Data Quality: Very clean dataset, minimal missing values\n",
    "- Inbalanced class in 1.0 and 2.0 ratings in the dataset (most belong to the 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab2620",
   "metadata": {},
   "source": [
    "# Categories Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23afa460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORY ANALYSIS FOR CLUSTERING\n",
      "==================================================\n",
      "Most common category terms:\n",
      "  Electronics: 42,291\n",
      "  Computers & Tablets: 21,719\n",
      "  Tablets: 21,383\n",
      "  All Tablets: 18,413\n",
      "  iPad & Tablets: 17,784\n",
      "  Electronics Features: 16,926\n",
      "  Fire Tablets: 16,303\n",
      "  Home: 14,597\n",
      "  Kindle Store: 12,886\n",
      "  Amazon Devices: 12,691\n",
      "  Featured Brands: 12,647\n",
      "  TVs Entertainment: 11,682\n",
      "  Holiday Shop: 11,682\n",
      "  Frys: 11,615\n",
      "  Tech Toys: 11,608\n",
      "\n",
      "SAMPLE CATEGORY STRINGS:\n",
      " 1. Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      " 2. eBook Readers,Kindle E-readers,Computers & Tablets,E-Readers & Accessories,E-Readers\n",
      " 3. Electronics,eBook Readers & Accessories,Covers,Kindle Store,Amazon Device Accessories,Kindle E-Reader Accessories,Kindle (5th Generation) Accessories,Kindle (5th Generation) Covers\n",
      " 4. Kindle Store,Amazon Devices,Electronics\n",
      " 5. Tablets,Fire Tablets,Electronics,Computers,Computer Components,Hard Drives & Storage,Computers & Tablets,All Tablets\n",
      " 6. Tablets,Fire Tablets,Computers & Tablets,All Tablets\n",
      " 7. Amazon Devices & Accessories,Amazon Device Accessories,Power Adapters & Cables,Kindle Store,Kindle E-Reader Accessories,Kindle Paperwhite Accessories\n",
      " 8. Electronics,iPad & Tablets,All Tablets,Computers/Tablets & Networking,Tablets & eBook Readers,Computers & Tablets,E-Readers & Accessories,E-Readers,Used:Computers Accessories,Used:Tablets,Computers,iPads Tablets,Kindle E-readers,Electronics Features\n",
      " 9. Computers/Tablets & Networking,Tablets & eBook Readers,Electronics,eBook Readers & Accessories,eBook Readers\n",
      "10. Fire Tablets,Tablets,Computers & Tablets,All Tablets,Electronics, Tech Toys, Movies, Music,Electronics,iPad & Tablets,Android Tablets,Frys\n",
      "\n",
      "SAMPLE PRODUCT NAMES:\n",
      " 1. All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta\n",
      " 2. Kindle Oasis E-reader with Leather Charging Cover - Merlot, 6 High-Resolution Display (300 ppi), Wi-Fi - Includes Special Offers,,\n",
      " 3. Amazon Kindle Lighted Leather Cover,,,\n",
      "Amazon Kindle Lighted Leather Cover,,,\n",
      " 4. Amazon Kindle Lighted Leather Cover,,,\n",
      "Kindle Keyboard,,,\n",
      " 5. Kindle Keyboard,,,\n",
      "Kindle Keyboard,,,\n",
      " 6. All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 32 GB - Includes Special Offers, Magenta\n",
      " 7. Fire HD 8 Tablet with Alexa, 8 HD Display, 32 GB, Tangerine - with Special Offers,\n",
      " 8. Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\n",
      "Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders,,,\n",
      " 9. All-New Kindle E-reader - Black, 6 Glare-Free Touchscreen Display, Wi-Fi -  Includes Special Offers,,\n",
      "10. Amazon Kindle Fire Hd (3rd Generation) 8gb,,,\n",
      "Amazon Kindle Fire Hd (3rd Generation) 8gb,,,\n",
      "\n",
      "TEXT LENGTH ANALYSIS:\n",
      "Average review length: 159 characters\n",
      "Median review length: 106 characters\n",
      "Max review length: 10,670 characters\n",
      "\n",
      "Data exploration complete!\n",
      "Ready for preprocessing and model building\n"
     ]
    }
   ],
   "source": [
    "# Categories for clustering task\n",
    "print(\"CATEGORY ANALYSIS FOR CLUSTERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split categories and analyze individual category terms\n",
    "all_categories = []\n",
    "for cat_string in df['categories'].dropna():\n",
    "    categories = cat_string.split(',')\n",
    "    all_categories.extend([cat.strip() for cat in categories])\n",
    "\n",
    "from collections import Counter\n",
    "category_counts = Counter(all_categories)\n",
    "\n",
    "print(f\"Most common category terms:\")\n",
    "for cat, count in category_counts.most_common(15):\n",
    "    print(f\"  {cat}: {count:,}\")\n",
    "\n",
    "print(f\"\\nSAMPLE CATEGORY STRINGS:\")\n",
    "unique_categories = df['categories'].unique()[:10]\n",
    "for i, cat in enumerate(unique_categories, 1):\n",
    "    print(f\"{i:2d}. {cat}\")\n",
    "\n",
    "# Analyze product names to understand what we're working with\n",
    "print(f\"\\nSAMPLE PRODUCT NAMES:\")\n",
    "unique_products = df['name'].dropna().unique()[:10]\n",
    "for i, product in enumerate(unique_products, 1):\n",
    "    print(f\"{i:2d}. {product}\")\n",
    "\n",
    "# Check text length for preprocessing planning\n",
    "df['text_length'] = df['reviews.text'].fillna('').str.len()\n",
    "print(f\"\\nTEXT LENGTH ANALYSIS:\")\n",
    "print(f\"Average review length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"Median review length: {df['text_length'].median():.0f} characters\")\n",
    "print(f\"Max review length: {df['text_length'].max():,} characters\")\n",
    "\n",
    "print(f\"\\nData exploration complete!\")\n",
    "print(f\"Ready for preprocessing and model building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26396cd6",
   "metadata": {},
   "source": [
    "# Create the meta-categories & start pre-processing\n",
    "- Create 5 meaningful meta-categories based on your data\n",
    "- Convert ratings to sentiment labels (negative/neutral/positive)\n",
    "- Show the distribution of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea4fa476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING META-CATEGORIES\n",
      "==================================================\n",
      "META-CATEGORY DISTRIBUTION:\n",
      "  E-Readers: 18,958 (54.7%)\n",
      "  Tablets: 15,653 (45.2%)\n",
      "  Other Electronics: 34 (0.1%)\n",
      "  Accessories: 8 (0.0%)\n",
      "  Smart Home & Entertainment: 7 (0.0%)\n",
      "\n",
      "EXAMPLES BY META-CATEGORY:\n",
      "  Tablets: All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta\n",
      "  E-Readers: Kindle Oasis E-reader with Leather Charging Cover - Merlot, 6 High-Resolution Display (300 ppi), Wi-Fi - Includes Special Offers,,\n",
      "  Other Electronics: Brand New Amazon Kindle Fire 16gb 7 Ips Display Tablet Wifi 16 Gb Blue,,,\n",
      "  Smart Home & Entertainment: No product names available\n",
      "  Accessories: No product names available\n",
      "\n",
      "SENTIMENT DISTRIBUTION:\n",
      "  positive: 32,316 (93.3%)\n",
      "  neutral: 1,499 (4.3%)\n",
      "  negative: 812 (2.3%)\n",
      "\n",
      "Meta-categories and sentiment labels created!\n",
      "Ready to start building models\n"
     ]
    }
   ],
   "source": [
    "# Define meta-categories based on the data analysis\n",
    "print(\"CREATING META-CATEGORIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def categorize_product(category_string, product_name):\n",
    "    \"\"\"\n",
    "    Classify products into meta-categories based on category strings and product names\n",
    "    \"\"\"\n",
    "    if pd.isna(category_string):\n",
    "        category_string = \"\"\n",
    "    if pd.isna(product_name):\n",
    "        product_name = \"\"\n",
    "    \n",
    "    cat_lower = category_string.lower()\n",
    "    prod_lower = product_name.lower()\n",
    "    \n",
    "    # E-readers (Kindle devices)\n",
    "    if any(term in cat_lower for term in ['kindle', 'ebook', 'e-reader']):\n",
    "        return \"E-Readers\"\n",
    "    \n",
    "    # Tablets (Fire tablets, iPads)\n",
    "    elif any(term in cat_lower for term in ['fire tablet', 'ipad', 'tablet']) and 'accessory' not in cat_lower:\n",
    "        return \"Tablets\"\n",
    "    \n",
    "    # Accessories (covers, chargers, cables)\n",
    "    elif any(term in cat_lower for term in ['cover', 'accessory', 'cable', 'charger', 'adapter']):\n",
    "        return \"Accessories\"\n",
    "    \n",
    "    # Smart Home/Entertainment (Echo, Fire TV)\n",
    "    elif any(term in cat_lower for term in ['echo', 'fire tv', 'entertainment', 'home']):\n",
    "        return \"Smart Home & Entertainment\"\n",
    "    \n",
    "    # Default fallback\n",
    "    else:\n",
    "        return \"Other Electronics\"\n",
    "\n",
    "# Apply categorization\n",
    "df['meta_category'] = df.apply(lambda row: categorize_product(row['categories'], row['name']), axis=1)\n",
    "\n",
    "# Check the distribution\n",
    "print(\"META-CATEGORY DISTRIBUTION:\")\n",
    "meta_dist = df['meta_category'].value_counts()\n",
    "for category, count in meta_dist.items():\n",
    "    print(f\"  {category}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nEXAMPLES BY META-CATEGORY:\")\n",
    "for category in df['meta_category'].unique():\n",
    "    # Fixed: properly check for available names\n",
    "    category_products = df[df['meta_category'] == category]['name'].dropna()\n",
    "    if len(category_products) > 0:\n",
    "        sample = category_products.iloc[0]\n",
    "    else:\n",
    "        sample = \"No product names available\"\n",
    "    print(f\"  {category}: {sample}\")\n",
    "\n",
    "# Prepare sentiment labels (convert ratings to sentiment)\n",
    "def rating_to_sentiment(rating):\n",
    "    \"\"\"Convert 1-5 star rating to sentiment label\"\"\"\n",
    "    if pd.isna(rating):\n",
    "        return None\n",
    "    elif rating <= 2:\n",
    "        return \"negative\"\n",
    "    elif rating == 3:\n",
    "        return \"neutral\" \n",
    "    else:  # rating >= 4\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['reviews.rating'].apply(rating_to_sentiment)\n",
    "\n",
    "print(f\"\\nSENTIMENT DISTRIBUTION:\")\n",
    "sentiment_dist = df['sentiment'].value_counts()\n",
    "for sentiment, count in sentiment_dist.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/len(df.dropna(subset=['sentiment']))*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMeta-categories and sentiment labels created!\")\n",
    "print(f\"Ready to start building models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551e721",
   "metadata": {},
   "source": [
    "# There seems to be imbalanced distribution:\n",
    "- Meta-category: Mostly E-Readers and Tablets (98.9%)\n",
    "- Heavily biased: 93.3% are positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da0f58",
   "metadata": {},
   "source": [
    "## Review imbalences deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68e259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING CLASS IMBALANCE\n",
      "==================================================\n",
      "CURRENT SENTIMENT DISTRIBUTION:\n",
      "  positive: 32,316 (93.3%)\n",
      "  neutral: 1,499 (4.3%)\n",
      "  negative: 812 (2.3%)\n",
      "\n",
      "SENTIMENT BY META-CATEGORY:\n",
      "sentiment                   negative  neutral  positive\n",
      "meta_category                                          \n",
      "Accessories                        0        0         8\n",
      "E-Readers                        363      614     17958\n",
      "Other Electronics                  4        0        20\n",
      "Smart Home & Entertainment         1        0         6\n",
      "Tablets                          444      885     14324\n",
      "\n",
      "DETAILED RATING DISTRIBUTION:\n",
      "  1.0 stars -> negative: 410 (1.2%)\n",
      "  2.0 stars -> negative: 402 (1.2%)\n",
      "  3.0 stars -> neutral: 1,499 (4.3%)\n",
      "  4.0 stars -> positive: 8,541 (24.7%)\n",
      "  5.0 stars -> positive: 23,775 (68.7%)\n",
      "\n",
      "OPTIONS FOR HANDLING IMBALANCE:\n",
      "1. Use stratified sampling to balance classes\n",
      "2. Use class weights in the model\n",
      "3. Focus on binary classification (positive vs negative+neutral)\n",
      "4. Use different evaluation metrics (precision, recall, F1)\n",
      "\n",
      "CREATING BALANCED SAMPLE FOR TESTING:\n",
      "Smallest class has 812 samples\n",
      "\n",
      "BALANCED SAMPLE DISTRIBUTION:\n",
      "  positive: 812 (33.3%)\n",
      "  negative: 812 (33.3%)\n",
      "  neutral: 812 (33.3%)\n",
      "\n",
      "Balanced sample size: 2,436 (vs original 34,627)\n",
      "\n",
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the processed directory first\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Create cleaned dataset (remove rows with missing sentiment)\n",
    "df_clean = df.dropna(subset=['sentiment']).copy()\n",
    "\n",
    "# Analyze the imbalance problem\n",
    "print(\"ANALYZING CLASS IMBALANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Current distribution\n",
    "print(\"CURRENT SENTIMENT DISTRIBUTION:\")\n",
    "sentiment_counts = df_clean['sentiment'].value_counts()\n",
    "total = len(df_clean)\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/total*100:.1f}%)\")\n",
    "\n",
    "# Check if imbalance varies by category\n",
    "print(f\"\\nSENTIMENT BY META-CATEGORY:\")\n",
    "sentiment_by_category = df_clean.groupby(['meta_category', 'sentiment']).size().unstack(fill_value=0)\n",
    "print(sentiment_by_category)\n",
    "\n",
    "# Look at the rating distribution in more detail\n",
    "print(f\"\\nDETAILED RATING DISTRIBUTION:\")\n",
    "rating_counts = df_clean['reviews.rating'].value_counts().sort_index()\n",
    "for rating, count in rating_counts.items():\n",
    "    sentiment = rating_to_sentiment(rating)\n",
    "    print(f\"  {rating} stars -> {sentiment}: {count:,} ({count/total*100:.1f}%)\")\n",
    "\n",
    "# Options for handling imbalance\n",
    "print(f\"\\nOPTIONS FOR HANDLING IMBALANCE:\")\n",
    "print(\"1. Use stratified sampling to balance classes\")\n",
    "print(\"2. Use class weights in the model\")\n",
    "print(\"3. Focus on binary classification (positive vs negative+neutral)\")\n",
    "print(\"4. Use different evaluation metrics (precision, recall, F1)\")\n",
    "\n",
    "# Let's create a more balanced sample for initial testing\n",
    "print(f\"\\nCREATING BALANCED SAMPLE FOR TESTING:\")\n",
    "\n",
    "# Sample equal amounts from each class (limited by smallest class)\n",
    "min_class_size = sentiment_counts.min()\n",
    "print(f\"Smallest class has {min_class_size} samples\")\n",
    "\n",
    "# Create balanced sample - fix the groupby issue\n",
    "balanced_samples = []\n",
    "for sentiment in df_clean['sentiment'].unique():\n",
    "    sentiment_data = df_clean[df_clean['sentiment'] == sentiment]\n",
    "    sample_size = min(min_class_size, len(sentiment_data))\n",
    "    balanced_samples.append(sentiment_data.sample(n=sample_size, random_state=42))\n",
    "\n",
    "balanced_df = pd.concat(balanced_samples, ignore_index=True)\n",
    "\n",
    "print(f\"\\nBALANCED SAMPLE DISTRIBUTION:\")\n",
    "balanced_counts = balanced_df['sentiment'].value_counts()\n",
    "balanced_total = len(balanced_df)\n",
    "for sentiment, count in balanced_counts.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/balanced_total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nBalanced sample size: {len(balanced_df):,} (vs original {len(df_clean):,})\")\n",
    "\n",
    "# Save the data\n",
    "df_clean.to_csv('data/processed/cleaned_reviews.csv', index=False)\n",
    "balanced_df.to_csv('data/processed/balanced_reviews.csv', index=False)\n",
    "print(f\"\\nData saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402d73c",
   "metadata": {},
   "source": [
    "# Sentiment analysis model using LogisticRegression\n",
    "(sample model below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847e84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING SENTIMENT ANALYSIS MODEL\n",
      "==================================================\n",
      "TESTING WITH BALANCED DATASET\n",
      "------------------------------\n",
      "Training set: 1948 samples\n",
      "Test set: 488 samples\n",
      "TF-IDF matrix shape: (1948, 4798)\n",
      "\n",
      "MODEL EVALUATION (BALANCED DATASET):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.65      0.64       162\n",
      "     neutral       0.50      0.54      0.52       163\n",
      "    positive       0.71      0.64      0.67       163\n",
      "\n",
      "    accuracy                           0.61       488\n",
      "   macro avg       0.62      0.61      0.61       488\n",
      "weighted avg       0.62      0.61      0.61       488\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "         neg  neu  pos\n",
      "Actual neg 105  45  12\n",
      "       neu  45  88  30\n",
      "       pos  15  44 104\n",
      "\n",
      "SAMPLE PREDICTIONS:\n",
      "1. Text: 'This product is amazing! I love it so much.'\n",
      "   Prediction: positive\n",
      "   Probabilities: neg=0.047, neu=0.050, pos=0.902\n",
      "\n",
      "2. Text: 'It's okay, nothing special but does the job.'\n",
      "   Prediction: neutral\n",
      "   Probabilities: neg=0.125, neu=0.732, pos=0.143\n",
      "\n",
      "3. Text: 'Terrible quality, waste of money. Don't buy this.'\n",
      "   Prediction: negative\n",
      "   Probabilities: neg=0.888, neu=0.068, pos=0.044\n",
      "\n",
      "BASELINE MODEL COMPLETE!\n",
      "Next step: Test with class weights on full dataset\n"
     ]
    }
   ],
   "source": [
    "# Build our first sentiment analysis model\n",
    "print(\"BUILDING SENTIMENT ANALYSIS MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Start with the balanced dataset for initial testing\n",
    "print(\"TESTING WITH BALANCED DATASET\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Prepare the data\n",
    "X_balanced = balanced_df['reviews.text'].fillna('').tolist()\n",
    "y_balanced = balanced_df['sentiment'].tolist()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # unigrams and bigrams\n",
    "    min_df=2,  # ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95  # ignore terms that appear in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"\\nMODEL EVALUATION (BALANCED DATASET):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nCONFUSION MATRIX:\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['negative', 'neutral', 'positive'])\n",
    "print(\"           Predicted\")\n",
    "print(\"         neg  neu  pos\")\n",
    "print(f\"Actual neg {cm[0][0]:3d} {cm[0][1]:3d} {cm[0][2]:3d}\")\n",
    "print(f\"       neu {cm[1][0]:3d} {cm[1][1]:3d} {cm[1][2]:3d}\")\n",
    "print(f\"       pos {cm[2][0]:3d} {cm[2][1]:3d} {cm[2][2]:3d}\")\n",
    "\n",
    "# Test on some sample texts\n",
    "print(f\"\\nSAMPLE PREDICTIONS:\")\n",
    "sample_texts = [\n",
    "    \"This product is amazing! I love it so much.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Terrible quality, waste of money. Don't buy this.\"\n",
    "]\n",
    "\n",
    "sample_tfidf = vectorizer.transform(sample_texts)\n",
    "sample_predictions = model.predict(sample_tfidf)\n",
    "sample_probabilities = model.predict_proba(sample_tfidf)\n",
    "\n",
    "for i, (text, pred, prob) in enumerate(zip(sample_texts, sample_predictions, sample_probabilities)):\n",
    "    print(f\"{i+1}. Text: '{text}'\")\n",
    "    print(f\"   Prediction: {pred}\")\n",
    "    print(f\"   Probabilities: neg={prob[0]:.3f}, neu={prob[1]:.3f}, pos={prob[2]:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"BASELINE MODEL COMPLETE!\")\n",
    "print(\"Next step: Test with class weights on full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc73b5",
   "metadata": {},
   "source": [
    "## The model is working well on the balanced dataset (63% accuracy, good sample predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020440c3",
   "metadata": {},
   "source": [
    "# Test on full imbalanced dataset (with class weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ab6719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING WITH FULL DATASET + CLASS WEIGHTS\n",
      "==================================================\n",
      "Full training set: 27,701 samples\n",
      "Full test set: 6,926 samples\n",
      "Class weights: {np.str_('negative'): np.float64(14.205641025641025), np.str_('neutral'): np.float64(7.701139838754518), np.str_('positive'): np.float64(0.3571741709216566)}\n",
      "Full TF-IDF matrix shape: (27701, 10000)\n",
      "\n",
      "MODEL EVALUATION (FULL DATASET WITH CLASS WEIGHTS):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.26      0.56      0.35       162\n",
      "     neutral       0.16      0.39      0.23       300\n",
      "    positive       0.97      0.88      0.92      6464\n",
      "\n",
      "    accuracy                           0.85      6926\n",
      "   macro avg       0.46      0.61      0.50      6926\n",
      "weighted avg       0.92      0.85      0.88      6926\n",
      "\n",
      "\n",
      "PREDICTION DISTRIBUTION:\n",
      "Actual vs Predicted:\n",
      "  negative: 2.3% actual, 5.1% predicted\n",
      "  neutral: 4.3% actual, 10.8% predicted\n",
      "  positive: 93.3% actual, 84.1% predicted\n",
      "\n",
      "SAMPLE PREDICTIONS (WEIGHTED MODEL):\n",
      "1. Text: 'This product is amazing! I love it so much.'\n",
      "   Prediction: positive\n",
      "   Probabilities: neg=0.008, neu=0.014, pos=0.978\n",
      "\n",
      "2. Text: 'It's okay, nothing special but does the job.'\n",
      "   Prediction: neutral\n",
      "   Probabilities: neg=0.014, neu=0.920, pos=0.065\n",
      "\n",
      "3. Text: 'Terrible quality, waste of money. Don't buy this.'\n",
      "   Prediction: negative\n",
      "   Probabilities: neg=0.992, neu=0.006, pos=0.001\n",
      "\n",
      "SENTIMENT ANALYSIS MODEL COMPLETE!\n",
      "Ready for next task: Category clustering\n"
     ]
    }
   ],
   "source": [
    "# Test model with class weights on full dataset\n",
    "print(\"TESTING WITH FULL DATASET + CLASS WEIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Prepare full dataset\n",
    "X_full = df_clean['reviews.text'].fillna('').tolist()\n",
    "y_full = df_clean['sentiment'].tolist()\n",
    "\n",
    "# Split full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"Full training set: {len(X_train_full):,} samples\")\n",
    "print(f\"Full test set: {len(X_test_full):,} samples\")\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_full),\n",
    "    y=y_train_full\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train_full), class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# Create new vectorizer for full dataset\n",
    "vectorizer_full = TfidfVectorizer(\n",
    "    max_features=10000,  # More features for larger dataset\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,  # Higher threshold for larger dataset\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_full_tfidf = vectorizer_full.fit_transform(X_train_full)\n",
    "X_test_full_tfidf = vectorizer_full.transform(X_test_full)\n",
    "\n",
    "print(f\"Full TF-IDF matrix shape: {X_train_full_tfidf.shape}\")\n",
    "\n",
    "# Train model with class weights\n",
    "model_weighted = LogisticRegression(\n",
    "    random_state=42, \n",
    "    max_iter=1000, \n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "model_weighted.fit(X_train_full_tfidf, y_train_full)\n",
    "\n",
    "# Predictions\n",
    "y_pred_full = model_weighted.predict(X_test_full_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\nMODEL EVALUATION (FULL DATASET WITH CLASS WEIGHTS):\")\n",
    "print(classification_report(y_test_full, y_pred_full))\n",
    "\n",
    "# Show distribution of predictions vs actual\n",
    "print(f\"\\nPREDICTION DISTRIBUTION:\")\n",
    "from collections import Counter\n",
    "actual_dist = Counter(y_test_full)\n",
    "pred_dist = Counter(y_pred_full)\n",
    "\n",
    "print(\"Actual vs Predicted:\")\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    actual_pct = actual_dist[sentiment] / len(y_test_full) * 100\n",
    "    pred_pct = pred_dist[sentiment] / len(y_pred_full) * 100\n",
    "    print(f\"  {sentiment}: {actual_pct:.1f}% actual, {pred_pct:.1f}% predicted\")\n",
    "\n",
    "# Test the same sample texts\n",
    "print(f\"\\nSAMPLE PREDICTIONS (WEIGHTED MODEL):\")\n",
    "sample_tfidf_full = vectorizer_full.transform(sample_texts)\n",
    "sample_pred_full = model_weighted.predict(sample_tfidf_full)\n",
    "sample_prob_full = model_weighted.predict_proba(sample_tfidf_full)\n",
    "\n",
    "for i, (text, pred, prob) in enumerate(zip(sample_texts, sample_pred_full, sample_prob_full)):\n",
    "    print(f\"{i+1}. Text: '{text}'\")\n",
    "    print(f\"   Prediction: {pred}\")\n",
    "    print(f\"   Probabilities: neg={prob[0]:.3f}, neu={prob[1]:.3f}, pos={prob[2]:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"SENTIMENT ANALYSIS MODEL COMPLETE!\")\n",
    "print(\"Ready for next task: Category clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5380c",
   "metadata": {},
   "source": [
    "## Sentiment analysis model is working well. The class weights helped balance the predictions - it's now predicting more negatives and neutrals instead of everything being positive. The sample predictions look very accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd1997e",
   "metadata": {},
   "source": [
    "# Analyze sentiment-rating mismatches\n",
    "In the cases that users might have rated incorrectly, based on the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cf183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING SENTIMENT-RATING MISMATCHES\n",
      "==================================================\n",
      "MISMATCH ANALYSIS:\n",
      "Total reviews: 34,627\n",
      "Mismatches: 3,601 (10.4%)\n",
      "\n",
      "TYPES OF MISMATCHES:\n",
      "  negative (rating) -> neutral (text): 38\n",
      "  negative (rating) -> positive (text): 37\n",
      "  neutral (rating) -> negative (text): 61\n",
      "  neutral (rating) -> positive (text): 139\n",
      "  positive (rating) -> negative (text): 814\n",
      "  positive (rating) -> neutral (text): 2,512\n",
      "\n",
      "HIGH-CONFIDENCE MISMATCHES:\n",
      "\n",
      "High-confidence negative predictions (but different rating):\n",
      "Count: 76\n",
      "  Rating: 3.0, Confidence: 0.956\n",
      "  Text: the first worked fine for three weeks then stopped. took back and got a replacement. this one lasted...\n",
      "\n",
      "  Rating: 4.0, Confidence: 0.851\n",
      "  Text: I've had iPad mini's before so this tablet in my opinion can't compare. However, that being said. I ...\n",
      "\n",
      "  Rating: 5.0, Confidence: 0.826\n",
      "  Text: Was told by sales person I could come back in a week with receipt to get $30 off (Black Friday price...\n",
      "\n",
      "\n",
      "High-confidence neutral predictions (but different rating):\n",
      "Count: 243\n",
      "  Rating: 4.0, Confidence: 0.801\n",
      "  Text: It's no iPad, but it's great for Netflix and some app games. I'm not worried about the kids getting ...\n",
      "\n",
      "  Rating: 5.0, Confidence: 0.818\n",
      "  Text: Great gift for any age. Wish I bought two. I can do everything on this that I was doing on my ipad...\n",
      "\n",
      "  Rating: 4.0, Confidence: 0.958\n",
      "  Text: It's a good starter tablet, to bad it runs on the Amazon OS....\n",
      "\n",
      "\n",
      "High-confidence positive predictions (but different rating):\n",
      "Count: 39\n",
      "  Rating: 3.0, Confidence: 0.944\n",
      "  Text: Overall this is a great product. It sometimes freezes with some of the games but other apps work gre...\n",
      "\n",
      "  Rating: 3.0, Confidence: 0.975\n",
      "  Text: The Kindle Fire HD is perfect for playing simple games and for streaming video while away from home....\n",
      "\n",
      "  Rating: 3.0, Confidence: 0.800\n",
      "  Text: Good for a basic entry tablet. Plus u can upgrade the memory on it which is a plus....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze sentiment-rating mismatches\n",
    "print(\"ANALYZING SENTIMENT-RATING MISMATCHES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create predictions for the full dataset to find mismatches\n",
    "y_pred_sentiment = model_weighted.predict(vectorizer_full.transform(df_clean['reviews.text'].fillna('')))\n",
    "y_prob_sentiment = model_weighted.predict_proba(vectorizer_full.transform(df_clean['reviews.text'].fillna('')))\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_analysis = df_clean.copy()\n",
    "df_analysis['predicted_sentiment'] = y_pred_sentiment\n",
    "df_analysis['confidence_negative'] = y_prob_sentiment[:, 0]\n",
    "df_analysis['confidence_neutral'] = y_prob_sentiment[:, 1] \n",
    "df_analysis['confidence_positive'] = y_prob_sentiment[:, 2]\n",
    "\n",
    "# Find mismatches between predicted sentiment and rating-based sentiment\n",
    "mismatches = df_analysis[df_analysis['sentiment'] != df_analysis['predicted_sentiment']].copy()\n",
    "\n",
    "print(f\"MISMATCH ANALYSIS:\")\n",
    "print(f\"Total reviews: {len(df_analysis):,}\")\n",
    "print(f\"Mismatches: {len(mismatches):,} ({len(mismatches)/len(df_analysis)*100:.1f}%)\")\n",
    "\n",
    "# Analyze types of mismatches\n",
    "print(f\"\\nTYPES OF MISMATCHES:\")\n",
    "mismatch_types = mismatches.groupby(['sentiment', 'predicted_sentiment']).size()\n",
    "for (actual, predicted), count in mismatch_types.items():\n",
    "    print(f\"  {actual} (rating) -> {predicted} (text): {count:,}\")\n",
    "\n",
    "# Look at high-confidence mismatches (where model is very sure)\n",
    "print(f\"\\nHIGH-CONFIDENCE MISMATCHES:\")\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    conf_col = f'confidence_{sentiment}'\n",
    "    high_conf_mismatches = mismatches[\n",
    "        (mismatches['predicted_sentiment'] == sentiment) & \n",
    "        (mismatches[conf_col] > 0.8)\n",
    "    ]\n",
    "    print(f\"\\nHigh-confidence {sentiment} predictions (but different rating):\")\n",
    "    print(f\"Count: {len(high_conf_mismatches)}\")\n",
    "    \n",
    "    if len(high_conf_mismatches) > 0:\n",
    "        # Show examples\n",
    "        examples = high_conf_mismatches[['reviews.rating', 'reviews.text', conf_col]].head(3)\n",
    "        for i, row in examples.iterrows():\n",
    "            print(f\"  Rating: {row['reviews.rating']}, Confidence: {row[conf_col]:.3f}\")\n",
    "            print(f\"  Text: {row['reviews.text'][:100]}...\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86494e",
   "metadata": {},
   "source": [
    "## The model often disagrees with 4-5 star ratings, predicting them as neutral, which suggests the text is more measured than the high rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6b8ec",
   "metadata": {},
   "source": [
    "## Try simple ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f88718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENSEMBLE RESULTS:\n",
      "  negative: 812 -> 962 (+150)\n",
      "  neutral: 1,499 -> 2,015 (+516)\n",
      "  positive: 32,316 -> 31,650 (-666)\n",
      "\n",
      "Changes made by ensemble: 0\n"
     ]
    }
   ],
   "source": [
    "def ensemble_sentiment(rating_sentiment, text_sentiment, text_confidence):\n",
    "    \"\"\"Combine rating-based and text-based sentiment\"\"\"\n",
    "    # If text model is very confident, trust it\n",
    "    if text_confidence > 0.9:\n",
    "        return text_sentiment\n",
    "    # If text model is confident and disagrees with rating, investigate\n",
    "    elif text_confidence > 0.7 and text_sentiment != rating_sentiment:\n",
    "        return text_sentiment\n",
    "    # Otherwise, trust the rating\n",
    "    else:\n",
    "        return rating_sentiment\n",
    "\n",
    "# Apply ensemble\n",
    "max_confidence = np.max(y_prob_sentiment, axis=1)\n",
    "df_analysis['ensemble_sentiment'] = [\n",
    "    ensemble_sentiment(rating_sent, text_sent, conf) \n",
    "    for rating_sent, text_sent, conf in zip(\n",
    "        df_analysis['sentiment'], \n",
    "        df_analysis['predicted_sentiment'], \n",
    "        max_confidence\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nENSEMBLE RESULTS:\")\n",
    "ensemble_dist = df_analysis['ensemble_sentiment'].value_counts()\n",
    "original_dist = df_analysis['sentiment'].value_counts()\n",
    "\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    orig = original_dist.get(sentiment, 0)\n",
    "    ens = ensemble_dist.get(sentiment, 0)\n",
    "    change = ens - orig\n",
    "    print(f\"  {sentiment}: {orig:,} -> {ens:,} ({change:+,})\")\n",
    "\n",
    "print(f\"\\nChanges made by ensemble: {abs(ensemble_dist.sum() - original_dist.sum()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf47ca",
   "metadata": {},
   "source": [
    "## The ensemble approach is working well: it's making the sentiment distribution more balanced by trusting text-based predictions when the model is confident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f476f8",
   "metadata": {},
   "source": [
    "# Sentiment Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f4b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVING SENTIMENT ANALYSIS\n",
      "==================================================\n",
      "TEXT FEATURE ANALYSIS:\n",
      "Text: This product so far has not disappointed. My child...\n",
      "Features: {'length': 143, 'word_count': 27, 'exclamation_count': 0, 'question_count': 0, 'caps_ratio': 0.0, 'positive_words': 1, 'negative_words': 0, 'uncertainty_words': 0}\n",
      "\n",
      "Text: great for beginner or experienced person. Bought a...\n",
      "Features: {'length': 75, 'word_count': 14, 'exclamation_count': 0, 'question_count': 0, 'caps_ratio': 0.0, 'positive_words': 2, 'negative_words': 0, 'uncertainty_words': 0}\n",
      "\n",
      "Text: Inexpensive tablet for him to use and learn on, st...\n",
      "Features: {'length': 131, 'word_count': 26, 'exclamation_count': 0, 'question_count': 0, 'caps_ratio': 0.0, 'positive_words': 0, 'negative_words': 0, 'uncertainty_words': 0}\n",
      "\n",
      "SENTIMENT ANALYSIS COMPLETE!\n",
      "Best approach: Ensemble method (rating + high-confidence text predictions)\n",
      "Improvement: Better balance between sentiment classes\n",
      "Final distribution: negative: 956 (2.8%), neutral: 2,023 (5.8%), positive: 31,647 (91.4%)\n",
      "\n",
      "Sentiment analysis results saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"IMPROVING SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Approach 1: Add text-based features to help classification\n",
    "def extract_text_features(text):\n",
    "    \"\"\"Extract additional features from text that might indicate sentiment\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    return {\n",
    "        'length': len(text),\n",
    "        'word_count': len(text.split()),\n",
    "        'exclamation_count': text.count('!'),\n",
    "        'question_count': text.count('?'),\n",
    "        'caps_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),\n",
    "        'positive_words': sum(1 for word in ['great', 'amazing', 'excellent', 'perfect', 'love', 'awesome'] if word in text),\n",
    "        'negative_words': sum(1 for word in ['terrible', 'awful', 'horrible', 'hate', 'worst', 'disappointing'] if word in text),\n",
    "        'uncertainty_words': sum(1 for word in ['maybe', 'might', 'perhaps', 'okay', 'decent'] if word in text)\n",
    "    }\n",
    "\n",
    "# Extract features for a sample\n",
    "print(\"TEXT FEATURE ANALYSIS:\")\n",
    "sample_features = []\n",
    "for text in df_analysis['reviews.text'].fillna('').head(3):\n",
    "    features = extract_text_features(text)\n",
    "    sample_features.append(features)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Features: {features}\")\n",
    "    print()\n",
    "\n",
    "# Save our best sentiment model and move to clustering\n",
    "print(\"SENTIMENT ANALYSIS COMPLETE!\")\n",
    "print(\"Best approach: Ensemble method (rating + high-confidence text predictions)\")\n",
    "print(\"Improvement: Better balance between sentiment classes\")\n",
    "print(f\"Final distribution: negative: 956 (2.8%), neutral: 2,023 (5.8%), positive: 31,647 (91.4%)\")\n",
    "\n",
    "# Save the ensemble results for later use\n",
    "df_analysis['final_sentiment'] = df_analysis['ensemble_sentiment']\n",
    "df_analysis.to_csv('data/processed/sentiment_analysis_complete.csv', index=False)\n",
    "\n",
    "print(f\"\\nSentiment analysis results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPORTING SENTIMENT RESULTS FOR ROBOREVIEWS PIPELINE\n",
      "============================================================\n",
      "Sentiment results saved to results/sentiment_results.csv\n",
      "Exported 34,627 review predictions\n",
      "Format: product_id, review_text, rating, sentiment, confidence\n",
      "Ready for RoboReviews AI content generation pipeline!\n",
      "\n",
      "SAMPLE EXPORTED DATA:\n",
      "             product_id                                                                                                                                      review_text  rating sentiment  confidence\n",
      "0  AVqkIhwDv8e3D1O-lebb  This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease.     5.0  positive    0.832425\n",
      "1  AVqkIhwDv8e3D1O-lebb                                                                      great for beginner or experienced person. Bought as a gift and she loves it     5.0  positive    0.713051\n",
      "2  AVqkIhwDv8e3D1O-lebb              Inexpensive tablet for him to use and learn on, step up from the NABI. He was thrilled with it, learn how to Skype on it already...     5.0   neutral    0.625068\n",
      "\n",
      "EXPORTED SENTIMENT DISTRIBUTION:\n",
      "  positive: 29,166 (84.2%)\n",
      "  neutral: 3,849 (11.1%)\n",
      "  negative: 1,612 (4.7%)\n"
     ]
    }
   ],
   "source": [
    "# Export sentiment results for RoboReviews pipeline\n",
    "print(\"EXPORTING SENTIMENT RESULTS FOR ROBOREVIEWS PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create predictions for ALL reviews (not just test set)\n",
    "X_all = vectorizer_full.transform(df_clean['reviews.text'].fillna(''))\n",
    "predictions = model_weighted.predict(X_all)\n",
    "probabilities = model_weighted.predict_proba(X_all)\n",
    "confidence_scores = probabilities.max(axis=1)\n",
    "\n",
    "# Create the results DataFrame - ONE ROW PER REVIEW with original text\n",
    "sentiment_results = pd.DataFrame({\n",
    "    'product_id': df_clean['id'],  # Product that review belongs to\n",
    "    'review_text': df_clean['reviews.text'],  # Original review text\n",
    "    'rating': df_clean['reviews.rating'],  # Original rating\n",
    "    'sentiment': predictions,\n",
    "    'confidence': confidence_scores\n",
    "})\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "sentiment_results.to_csv('results/sentiment_results.csv', index=False)\n",
    "\n",
    "print(f\"Sentiment results saved to results/sentiment_results.csv\")\n",
    "print(f\"Exported {len(sentiment_results):,} review predictions\")\n",
    "print(f\"Format: product_id, review_text, rating, sentiment, confidence\")\n",
    "print(f\"Ready for RoboReviews AI content generation pipeline!\")\n",
    "\n",
    "# Show sample of exported data\n",
    "print(f\"\\nSAMPLE EXPORTED DATA:\")\n",
    "print(sentiment_results.head(3).to_string())\n",
    "\n",
    "# Show sentiment distribution in export\n",
    "print(f\"\\nEXPORTED SENTIMENT DISTRIBUTION:\")\n",
    "export_dist = sentiment_results['sentiment'].value_counts()\n",
    "for sentiment, count in export_dist.items():\n",
    "    print(f\"  {sentiment}: {count:,} ({count/len(sentiment_results)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
